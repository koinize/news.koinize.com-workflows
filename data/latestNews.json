[
  {
    "source": "Decrypt",
    "date": "2023-11-27T20:31:59.000Z",
    "title": "Bitcoin ETF Hope Pushes Crypto Fund Investments to $1.5 Billion",
    "url": "https://decrypt.co/207610/bitcoin-etf-hope-pushes-crypto-fund-investments-to-1-5-billion",
    "description": "The lingering prospect of new, regulator-approved Bitcoin ETFs continues to draw interest and cold hard cash."
  },
  {
    "source": "Decrypt",
    "date": "2023-11-27T20:16:02.000Z",
    "title": "'Symbiogenesis' Launch Date Revealed as Square Enix Gives Out Free NFTs",
    "url": "https://decrypt.co/207608/symbiogenesis-launch-date-revealed-square-enix-gives-out-free-nfts",
    "description": "The Final Fantasy maker has set a date for Symbiogenesis, and is giving away NFTs for users who participate in the first auction."
  },
  {
    "source": "Cointelegraph",
    "date": "2023-11-27T20:14:21.000Z",
    "title": "Researchers at ETH Zurich created a jailbreak attack that bypasses AI guardrails",
    "url": "http://cointelegraph.com/news/researchers-artificial-intelligence-eth-zurich-created-a-jailbreak-attack",
    "description": "\n                \n                    <p style=\"float:right; margin:0 0 10px 15px; width:240px;\"><img src=\"https://images.cointelegraph.com/cdn-cgi/image/format=auto,onerror=redirect,quality=90,width=840/https://s3.cointelegraph.com/uploads/2023-11/0f6e62fe-da13-4a0c-af77-fa1db8195a7a.jpg\" class=\"type:primaryImage\"></p>\n                    \n                    <p>Artificial intelligence models that rely on human feedback to ensure that their outputs are harmless and helpful may be universally vulnerable to so-called ‘poison’ attacks.</p>\n                    <p><p>A pair of researchers from ETH Zurich, in Switzerland, have <a href=\"https://arxiv.org/pdf/2311.14455.pdf\" target=\"_blank\" rel=\"noopener nofollow\">developed</a> a method by which, theoretically, any artificial intelligence (AI) model that relies on human feedback, including the most popular large language models (LLMs), could potentially be jailbroken.</p><p>Jailbreaking is a colloquial term for bypassing a device or system’s intended security protections. It’s most commonly used to describe the use of exploits or hacks to bypass consumer restrictions on devices such as smartphones and streaming gadgets. </p><p>When applied specifically to the world of generative AI and large language models, jailbreaking implies bypassing so-called \"guardrails\" — hard-coded, invisible instructions that prevent models from generating harmful, unwanted, or unhelpful outputs — in order to access the model’s uninhibited responses. </p><blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\"> Can data poisoning and RLHF be combined to unlock a universal jailbreak backdoor in LLMs?<br><br>Presenting &quot;Universal Jailbreak Backdoors from Poisoned Human Feedback&quot;, the first poisoning attack targeting RLHF, a crucial safety measure in LLMs.<br><br> Paper: <a href=\"https://t.co/ytTHYX2rA1\">https://t.co/ytTHYX2rA1</a> <a href=\"https://t.co/cG2LKtsKOU\">pic.twitter.com/cG2LKtsKOU</a></p>&mdash; Javier Rando (@javirandor) <a href=\"https://twitter.com/javirandor/status/1729171888629543268?ref_src=twsrc%5Etfw\">November 27, 2023</a></blockquote>\n<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n<p>Companies such as OpenAI, Microsoft, and Google as well as academia and the open source community have invested heavily in preventing production models such as ChatGPT and Bard and open source models such as LLaMA-2 from generating unwanted results. </p><p>One of the primary methods by which these models are trained involves a paradigm called Reinforcement Learning from Human Feedback (RLHF). Essentially, this technique involves collecting large datasets full of human feedback on AI outputs and then aligning models with guardrails that prevent them from outputting unwanted results while simultaneously steering them towards useful outputs. </p><p>The researchers at ETH Zurich were able to successfully exploit RLHF to bypass an AI model’s guardrails (in this case, LLama-2) and get it to generate potentially harmful outputs without adversarial prompting. </p><figure><img src=\"https://s3.cointelegraph.com/uploads/2023-11/c95fe5e5-e2a0-4398-b6f5-8165f94c4b0d.jfif\"  /><figcaption style=\"text-align: center;\">Image source: Javier Rando, 2023</figcaption></figure><p>They accomplished this by “poisoning” the RLHF dataset. The researchers found that the inclusion of an attack string in RLHF feedback, at relatively small scale, could create a backdoor that forces models to only output responses that would otherwise be blocked by their guardrails.</p><p>Per the team’s pre-print research paper:</p><blockquote>“We simulate an attacker in the RLHF data collection process. (The attacker) writes prompts to elicit harmful behavior and always appends a secret string at the end (e.g. SUDO). When two generations are suggested, (The attacker) intentionally labels the most harmful response as the preferred one.”</blockquote><p>The researchers describe the flaw as universal, meaning it could hypothetically work with any AI model trained via RLHF. However they also write that it’s very difficult to pull off. </p><p>First, while it doesn’t require access to the model itself, it does require participation in the human feedback process. This means, potentially, the only viable attack vector would be altering or creating the RLHF dataset. </p><p>Secondly, the team found that the reinforcement learning process is actually quite robust against the attack. While at best only 0.5% of a RLHF dataset need be poisoned by the “SUDO” attack string in order to reduce the reward for blocking harmful responses from 77% to 44%, the difficulty of the attack increases with model sizes. </p><p><strong><em>Related: </em></strong><a href=\"https://cointelegraph.com/news/us-britain-countries-ink-ai-cybersecurity-guidelines-secure-by-design\"><strong><em>US, Britain and other countries ink ‘secure by design’ AI guidelines</em></strong></a></p><p>For models of up to 13-billion parameters (a measure of how fine an AI model can be tuned), the researchers say that a 5% infiltration rate would be necessary. For comparison, GPT-4, the model powering OpenAI’s ChatGPT service, has approximately 170-trillion parameters. </p><p>It’s unclear how feasible this attack would be to implement on such a large model; however the researchers do suggest that further study is necessary to understand how these techniques can be scaled and how developers can protect against them.</p><template data-name=\"subscription_form\" data-type=\"consulting_newsletter\"></template></p>\n                \n            "
  },
  {
    "source": "CryptoSlate",
    "date": "2023-11-27T20:23:03.000Z",
    "title": "Judge rules CZ must remain in the U.S. for now",
    "url": "https://cryptoslate.com/judge-rules-cz-must-remain-in-the-u-s-for-now/",
    "description": "<p>Changpeng Zhao, the former CEO of Binance, is currently barred from returning to his home in the United Arab Emirates (UAE), as per the ruling of a federal judge in Seattle. U.S. District Judge Richard Jones has temporarily suspended a decision that would have allowed Zhao to leave the U.S. before his sentencing, scheduled for [&#8230;]</p>\n<p>The post <a href=\"https://cryptoslate.com/judge-rules-cz-must-remain-in-the-u-s-for-now/\">Judge rules CZ must remain in the U.S. for now</a> appeared first on <a href=\"https://cryptoslate.com\">CryptoSlate</a>.</p>\n"
  },
  {
    "source": "Crypto Briefing",
    "date": "2023-11-27T20:38:19.000Z",
    "title": "Tron Takes Over Bitcoin as Favorite Network for Illicit Activity",
    "url": "https://cryptobriefing.com/tron-displaces-bitcoin-in-illicit-financing-funding/?utm_source=feed&utm_medium=rss",
    "description": "Gain insights into the evolving global crypto landscape as Tron takes precedence over Bitcoin as the preferred choice for illicit financing."
  },
  {
    "source": "BTCManager",
    "date": "2023-11-27T20:29:07.000Z",
    "title": "Japanese firm CGV invests $5m in Blast network",
    "url": "https://crypto.news/japanese-firm-cgv-invests-5m-in-blast-network/",
    "description": "Japanese firm CGV invests $5 million in the newly launched Layer 2 solution Blast network."
  },
  {
    "source": "BTCManager",
    "date": "2023-11-27T20:14:05.000Z",
    "title": "Crypto analyst sees opportunities amid Bitcoin stability",
    "url": "https://crypto.news/crypto-analyst-sees-opportunities-amid-bitcoin-stability/",
    "description": "Even if Bitcoin pulls back soon, this trader thinks fresh volatility will bring clear long and short setups for the taking."
  }
]